# DemonstraÈ›ie ReÈ›ele Neuronale & Backpropagation ğŸ§ âš¡

> **Lucrare pentru Sesiunea de ComunicÄƒri È˜tiinÈ›ifice StudenÈ›eÈ™ti 2025**  
> **Universitatea RomÃ¢no-AmericanÄƒ | Facultatea de InformaticÄƒ ManagerialÄƒ**

![Project Status](https://img.shields.io/badge/Status-Finalizat-success)
![Version](https://img.shields.io/badge/Version-v2.0-blue)
![License](https://img.shields.io/badge/License-MIT-purple)

O aplicaÈ›ie interactivÄƒ conceputÄƒ pentru a demistifica conceptul de "Black Box" al reÈ›elelor neuronale. Proiectul oferÄƒ o vizualizare graficÄƒ Ã®n timp real a arhitecturii Perceptronului Multistrat (MLP), demonstrÃ¢nd procesul de ajustare a ponderilor sinaptice prin algoritmul **Backpropagation** pe problema neliniarÄƒ XOR.

---

## ğŸ‘¥ Echipa de Proiect

**Autori (Grupa 624):**

* ğŸ“ **Octavian Mihai**
* ğŸ“ **Abbasi Pazeyazd Bianca-Maria**

**Profesor Coordonator:**

* ğŸ‘©â€ğŸ« **Prof. Univ. Dr. Gruiescu Mihaela** (Disciplina: StatisticÄƒ)

---

## âœ¨ FuncÈ›ionalitÄƒÈ›i Cheie

### 1. ğŸ” Vizualizare ArhitecturÄƒ NeuralÄƒ

ObservÄƒ Ã®n timp real conexiunile dintre neuroni.

* **Ponderi Dinamice**: Grosimea È™i opacitatea liniilor reflectÄƒ magnitudinea ponderilor sinaptice.
* **Cod Culori**: Ponderile pozitive È™i negative sunt diferenÈ›iate cromatic pentru o analizÄƒ rapidÄƒ.

### 2. ğŸ“‰ Grafice & Metrici Live

* **Loss Graph**: UrmÄƒreÈ™te minimizarea erorii (MSE - Mean Squared Error) epocÄƒ cu epocÄƒ.
* **Decision Boundary**: O hartÄƒ termicÄƒ ce aratÄƒ cum reÈ›eaua segmenteazÄƒ spaÈ›iul decizional 2D.

### 3. ğŸ›ï¸ Laborator Experimental

Ai control total asupra parametrilor reÈ›elei:

* **ArhitecturÄƒ FlexibilÄƒ**: AdaugÄƒ sau eliminÄƒ straturi ascunse È™i configureazÄƒ numÄƒrul de neuroni.
* **Hiperparametri**: AjusteazÄƒ Rata de ÃnvÄƒÈ›are (Learning Rate) pentru a vedea impactul asupra convergenÈ›ei.
* **Antrenare VitezÄƒ VariabilÄƒ**: ControleazÄƒ viteza simulÄƒrii pentru a observa detaliile fine.

### 4. ğŸ“š DocumentaÈ›ie IntegratÄƒ

* **InterfaÈ›Äƒ AcademicÄƒ**: Termeni tehnici riguroÈ™i È™i explicaÈ›ii matematice.
* **Ghid Interactiv**: InstrucÈ›iuni pas-cu-pas direct Ã®n aplicaÈ›ie.
* **FAQ**: RÄƒspunsuri la Ã®ntrebÄƒri despre overfitting, funcÈ›ii de activare È™i normalizare.

---

## ğŸ› ï¸ Tehnologii Utilizate

* **Frontend**: [React](https://react.dev/) + [Vite](https://vitejs.dev/)
* **Languages**: JavaScript (ES6+)
* **Styling**: Vanila CSS (Design Cyberpunk/Academic)
* **Libraries**: [Lucide React](https://lucide.dev/) (Icons)
* **Neural Engine**: **Custom Pure JS Implementation** (No external ML libs like TensorFlow) - DemonstrÃ¢nd Ã®nÈ›elegerea profundÄƒ a algoritmilor.

---

## ğŸš€ Instalare È™i Rulare

DacÄƒ doreÈ™ti sÄƒ rulezi proiectul local:

1. **CloneazÄƒ repository-ul:**

    ```bash
    git clone https://github.com/mihaigoctavian24/retea-neuronala.git
    cd retea-neuronala
    ```

2. **InstaleazÄƒ dependenÈ›ele:**

    ```bash
    npm install
    ```

3. **PorneÈ™te serverul de dezvoltare:**

    ```bash
    npm run dev
    ```

    AplicaÈ›ia va fi disponibilÄƒ la `http://localhost:5173`.

---

## ğŸ“ Fundamente Teoretice

Procesul de antrenare se bazeazÄƒ pe algoritmul **Gradient Descent** cu **Backpropagation**.

**Arhitectura de bazÄƒ:**

* **Input Layer**: 2 neuroni (XOR inputs)
* **Hidden Layers**: Sigmoid Activation Function
* **Output Layer**: 1 neuron (Class probability)

**Matematica din spate:**
Actualizarea ponderilor se face conform regulii delta:
> $$w_{new} = w_{old} - \eta \cdot \frac{\partial E}{\partial w}$$

Unde $\eta$ este rata de Ã®nvÄƒÈ›are.

---

## ğŸ“„ LicenÈ›Äƒ

Acest proiect este licenÈ›iat sub [MIT License](LICENSE).
Copyright Â© 2025 Octavian Mihai & Abassi Pazeyazd Bianca-Maria. All Rights Reserved.
