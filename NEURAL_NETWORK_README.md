# ğŸ§  ReÈ›ea NeuralÄƒ InteractivÄƒ - Vizualizare AnimatÄƒ

## ğŸ¯ Ce Este AceastÄƒ AplicaÈ›ie?

O aplicaÈ›ie web **React interactivÄƒ È™i animatÄƒ** care demonstreazÄƒ vizual cum funcÈ›ioneazÄƒ o reÈ›ea neuralÄƒ artificialÄƒ, pas cu pas! Vezi Ã®n timp real:
- ğŸ”µ Forward propagation animatÄƒ
- ğŸ¨ Neuroni care se "aprind" cÃ¢nd sunt activi
- ğŸŸ¢ğŸ”´ GreutÄƒÈ›i colorate (verde=pozitiv, roÈ™u=negativ)
- âš« Semnale animate care trec prin conexiuni
- ğŸ“Š Antrenare live pe XOR problem
- ğŸ® Controale interactive pentru experimentare

---

## ğŸ“¦ Ce PrimeÈ™ti

### FiÈ™ierul ArhivÄƒ: `neural-network-viz.tar.gz`

ConÈ›ine:
```
neural-network-viz/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx          # Componenta principalÄƒ + logicÄƒ NN (implementare de la zero!)
â”‚   â”œâ”€â”€ App.css          # Stilizare modernÄƒ cu animaÈ›ii
â”‚   â”œâ”€â”€ main.jsx         # Entry point React
â”‚   â””â”€â”€ index.css        # Stiluri globale
â”œâ”€â”€ public/              # Assets statice
â”œâ”€â”€ package.json         # DependenÈ›e (React, Vite)
â”œâ”€â”€ vite.config.js       # Configurare build
â””â”€â”€ index.html           # HTML template
```

---

## ğŸš€ Instalare È™i Rulare - 3 PaÈ™i Simpli!

### ğŸ“¥ Pas 1: Extrage Arhiva

```bash
# Extrage arhiva
tar -xzf neural-network-viz.tar.gz

# NavigheazÄƒ Ã®n folder
cd neural-network-viz
```

### ğŸ“¦ Pas 2: InstaleazÄƒ DependenÈ›ele

```bash
npm install
```

**Durata:** ~30 secunde (instaleazÄƒ React, Vite È™i cÃ¢teva dependenÈ›e)

### â–¶ï¸ Pas 3: RuleazÄƒ AplicaÈ›ia

```bash
npm run dev
```

**Output aÈ™teptat:**
```
  VITE v6.0.7  ready in 234 ms

  âœ  Local:   http://localhost:5173/
  âœ  Network: use --host to expose
  âœ  press h + enter to show help
```

Deschide browserul la `http://localhost:5173` È™i **BOOM!** ğŸ‰

---

## ğŸ® Cum sÄƒ FoloseÈ™ti AplicaÈ›ia

### 1ï¸âƒ£ Antrenarea ReÈ›elei (XOR Problem)

**Ce este XOR?** 
FuncÈ›ia logicÄƒ: `0âŠ•0=0`, `0âŠ•1=1`, `1âŠ•0=1`, `1âŠ•1=0`

**Tutorial Rapid:**
1. Click pe butonul **"â–¶ï¸ Start Antrenare"**
2. PriveÈ™te cum:
   - **Loss scade** (de la ~1.0 cÄƒtre ~0.0)
   - **AnimaÈ›iile** aratÄƒ semnale trecÃ¢nd prin reÈ›ea
   - **GreutÄƒÈ›ile se ajusteazÄƒ** (culori È™i grosimi se schimbÄƒ)
3. DupÄƒ ~1000 epoci, reÈ›eaua a Ã®nvÄƒÈ›at XOR perfect!
4. Click **"â¸ï¸ PauzÄƒ"** pentru a opri

**Controale Disponibile:**
- **Learning Rate Slider (0.01-1.0):**
  - Mic (0.1-0.3): ÃnvÄƒÈ›are lentÄƒ dar stabilÄƒ
  - Mare (0.5-0.9): ÃnvÄƒÈ›are rapidÄƒ dar riscantÄƒ
- **AratÄƒ greutÄƒÈ›ile:** Checkbox pentru valori numerice pe conexiuni
- **ğŸ”„ Reset:** ReiniÈ›ializeazÄƒ reÈ›eaua

### 2ï¸âƒ£ Testarea PredicÈ›iilor

DupÄƒ antrenare, testeazÄƒ reÈ›eaua:

1. FoloseÈ™te **sliderele Input 1 È™i Input 2** (valori 0-1)
2. Click **"ğŸ”® Prezice"**
3. Vezi **animaÈ›ia forward propagation** pas cu pas
4. Rezultat: **PredicÈ›ia** + **bara de progres vizualÄƒ**

**Teste Validate XOR:**
```
Input [0, 0] â†’ Output ~0.00 âœ… (XOR = 0)
Input [0, 1] â†’ Output ~0.99 âœ… (XOR = 1)
Input [1, 0] â†’ Output ~0.98 âœ… (XOR = 1)
Input [1, 1] â†’ Output ~0.02 âœ… (XOR = 0)
```

### 3ï¸âƒ£ ÃnÈ›elegerea VizualizÄƒrii

**Cod Vizual:**

| Element | SemnificaÈ›ie |
|---------|-------------|
| ğŸŸ¢ **Conexiune Verde** | Greutate pozitivÄƒ (amplificÄƒ semnalul) |
| ğŸ”´ **Conexiune RoÈ™ie** | Greutate negativÄƒ (suprimÄƒ semnalul) |
| **Linie GroasÄƒ** | Greutate mare (impact puternic) |
| **Linie SubÈ›ire** | Greutate micÄƒ (impact redus) |
| ğŸ”µ **Neuron Albastru Intens** | Activare puternicÄƒ (~1.0) |
| âšª **Neuron Alb** | Activare slabÄƒ (~0.0) |
| âš« **Punct Animat** | Semnal care trece prin conexiune |

**Arhitectura:** 2 â†’ 4 â†’ 1
- **2 neuroni input:** xâ‚, xâ‚‚ (cele douÄƒ valori de intrare)
- **4 neuroni hidden:** hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„ (Ã®nvaÈ›Äƒ pattern-uri complexe)
- **1 neuron output:** y (predicÈ›ia finalÄƒ)

---

## ğŸ’» Cum FuncÈ›ioneazÄƒ Sub CapotÄƒ

### Implementare de la Zero!

**Nu foloseÈ™te TensorFlow sau PyTorch** - totul scris Ã®n JavaScript pur pentru Ã®nvÄƒÈ›are:

```javascript
class NeuralNetwork {
  constructor(inputSize, hiddenSize, outputSize) {
    // IniÈ›ializare greutÄƒÈ›i random
    this.weightsIH = randomMatrix(inputSize, hiddenSize);
    this.weightsHO = randomMatrix(hiddenSize, outputSize);
    this.biasH = randomMatrix(1, hiddenSize);
    this.biasO = randomMatrix(1, outputSize);
  }
  
  forward(inputs) {
    // Input â†’ Hidden (cu sigmoid)
    const hidden = this.layer(inputs, this.weightsIH, this.biasH);
    
    // Hidden â†’ Output (cu sigmoid)
    const outputs = this.layer(hidden, this.weightsHO, this.biasO);
    
    return outputs;
  }
  
  train(inputs, targets, learningRate) {
    // Forward pass
    const outputs = this.forward(inputs);
    
    // Backpropagation (calcul gradienÈ›i)
    const outputErrors = targets - outputs;
    const hiddenErrors = /* calcul erori hidden */;
    
    // Update greutÄƒÈ›i cu gradient descent
    this.weightsHO += learningRate * gradient_HO;
    this.weightsIH += learningRate * gradient_IH;
  }
}
```

### Concepte Implementate:

1. **Forward Propagation:**
   ```
   Input â†’ [weights * inputs + bias] â†’ Sigmoid â†’ Hidden
   Hidden â†’ [weights * hidden + bias] â†’ Sigmoid â†’ Output
   ```

2. **FuncÈ›ia Sigmoid:**
   ```javascript
   sigmoid(x) = 1 / (1 + e^(-x))
   ```
   ComprimÄƒ orice valoare Ã®ntre 0 È™i 1

3. **Backpropagation:**
   - CalculeazÄƒ eroarea: `error = target - output`
   - PropagÄƒ eroarea Ã®napoi
   - AjusteazÄƒ greutÄƒÈ›ile: `weight += learningRate * gradient`

4. **Gradient Descent:**
   ```
   Î”w = Î± * âˆ‚Loss/âˆ‚w
   ```
   unde Î± = learning rate

---

## ğŸ“š Pentru Lucrarea VoastrÄƒ

### ğŸ¯ Integrare Ã®n Capitolul 3

**Plasament ideal:**
```
3. DEEP LEARNING È˜I REÈšELE NEURONALE
3.1. Arhitecturi Moderne
3.2. Optimizare È™i Backpropagation
â†’ 3.3. DemonstraÈ›ie InteractivÄƒ: Vizualizarea unei ReÈ›ele Neuronale â† AICI!
    3.3.1. Arhitectura ImplementatÄƒ (2-4-1)
    3.3.2. Forward Propagation Vizualizat
    3.3.3. Procesul de Antrenare (XOR Problem)
    3.3.4. Rezultate È™i Interpretare
    3.3.5. Insights È™i LimitÄƒri
```

### ğŸ“¸ Screenshots EsenÈ›iale

CapteazÄƒ aceste momente:

1. **Ãnainte de antrenare:**
   - Loss = 1.0
   - GreutÄƒÈ›i random
   - PredicÈ›ii greÈ™ite

2. **Ãn timpul antrenÄƒrii:**
   - Loss Ã®n scÄƒdere
   - AnimaÈ›ii vizibile
   - Epoca ~500

3. **DupÄƒ antrenare:**
   - Loss < 0.1
   - GreutÄƒÈ›i optimizate
   - PredicÈ›ii corecte

4. **Testare live:**
   - Toate 4 cazuri XOR testate
   - PredicÈ›ii + bare de progres

5. **Dashboard:**
   - Controale vizibile
   - Metrici Ã®n timp real

### âœï¸ Text Pentru Lucrare

**Introducere SecÈ›iunea 3.3:**

```
Pentru a Ã®nÈ›elege intuitiv funcÈ›ionarea reÈ›elelor neuronale, am dezvoltat 
o aplicaÈ›ie web interactivÄƒ care vizualizeazÄƒ pas cu pas procesul de 
forward propagation È™i backpropagation. AplicaÈ›ia demonstreazÄƒ antrenarea 
unei reÈ›ele 2-4-1 pe problema XOR, un benchmark clasic Ã®n Ã®nvÄƒÈ›area automatÄƒ.

[INSERAÈšI SCREENSHOT: InterfaÈ›a aplicaÈ›iei]

Arhitectura constÄƒ din:
- 2 neuroni de intrare (xâ‚, xâ‚‚)
- 4 neuroni Ã®n stratul ascuns cu activare sigmoid
- 1 neuron de ieÈ™ire pentru predicÈ›ia finalÄƒ

Implementarea este realizatÄƒ integral Ã®n JavaScript, fÄƒrÄƒ biblioteci externe 
de machine learning, permiÈ›Ã¢nd Ã®nÈ›elegerea profundÄƒ a fiecÄƒrui pas algoritmic.
```

**CÃ¢nd discutaÈ›i Forward Propagation:**

```
Procesul poate fi observat vizual Ã®n aplicaÈ›ia interactivÄƒ (Fig. 3.3.1), 
unde semnalele animate aratÄƒ exact cum informaÈ›ia se propagÄƒ de la intrare 
cÄƒtre ieÈ™ire. Conexiunile sunt colorate Ã®n funcÈ›ie de greutate: verde pentru 
valori pozitive care amplificÄƒ semnalul, roÈ™u pentru valori negative care 
suprimÄƒ semnalul.

[INSERAÈšI SCREENSHOT: AnimaÈ›ia forward propagation]
```

**Pentru Backpropagation:**

```
DeÈ™i backpropagation nu este vizualizat explicit, efectul sÄƒu poate fi 
observat Ã®n timp real: greutÄƒÈ›ile se ajusteazÄƒ gradual, loss-ul scade 
constant, È™i predicÈ›iile devin din ce Ã®n ce mai precise. DupÄƒ ~1000 epoci 
de antrenare cu learning rate 0.5, reÈ›eaua atinge o acurateÈ›e de >95% 
pe problema XOR.

[INSERAÈšI SCREENSHOT: EvoluÈ›ia loss-ului]
```

**Concluzii SecÈ›iunea 3.3:**

```
Vizualizarea interactivÄƒ demonstreazÄƒ cÄƒ reÈ›elele neuronale, deÈ™i matematice 
complexe, funcÈ›ioneazÄƒ pe principii intuitive: ajustare iterativÄƒ a parametrilor 
pentru minimizarea erorii. Problema XOR, imposibil de rezolvat cu un perceptron 
simplu, devine trivialÄƒ cu un singur strat ascuns, ilustrÃ¢nd puterea 
non-linearitÄƒÈ›ii introduse de funcÈ›ia sigmoid.

LimitÄƒri observate:
- ConvergenÈ›a depinde critic de learning rate (valori >0.9 duc la oscilaÈ›ii)
- IniÈ›ializarea aleatoare poate influenÈ›a viteza de convergenÈ›Äƒ
- Pentru probleme mai complexe, sunt necesare arhitecturi mai profunde

ÃnsÄƒ pentru scopuri educaÈ›ionale, aceastÄƒ implementare oferÄƒ o Ã®nÈ›elegere 
fundamentalÄƒ a mecanicilor reÈ›elelor neuronale care se extind direct cÄƒtre 
arhitecturi moderne precum CNN-uri È™i Transformers.
```

---

## ğŸ› ï¸ ModificÄƒri È™i Experimentare

### 1. SchimbÄƒ Arhitectura

Ãn `src/App.jsx`, linia ~115:
```javascript
const [nn] = useState(() => new NeuralNetwork(2, 4, 1));
//                                           â†‘  â†‘  â†‘
//                                      input hidden output
```

**Experimente:**
- `(2, 2, 1)` - Minimal (poate sÄƒ nu Ã®nveÈ›e XOR!)
- `(2, 8, 1)` - Mai mulÈ›i neuroni (Ã®nvaÈ›Äƒ mai rapid)
- `(2, 4, 2)` - 2 outputuri (pentru clasificare multi-clasÄƒ)

### 2. Alte Probleme Logice

ÃnlocuieÈ™te `trainingData`:

**AND Logic:**
```javascript
const trainingData = [
  { input: [0, 0], target: [0] },
  { input: [0, 1], target: [0] },
  { input: [1, 0], target: [0] },
  { input: [1, 1], target: [1] }
];
```

**OR Logic:**
```javascript
const trainingData = [
  { input: [0, 0], target: [0] },
  { input: [0, 1], target: [1] },
  { input: [1, 0], target: [1] },
  { input: [1, 1], target: [1] }
];
```

**NOT (Inversare):**
```javascript
const trainingData = [
  { input: [0], target: [1] },
  { input: [1], target: [0] }
];
// Trebuie sÄƒ schimbi arhitectura Ã®n (1, 2, 1)
```

### 3. AjusteazÄƒ Viteza AnimaÈ›iei

Ãn `src/App.jsx`, linia ~327:
```javascript
}, 100); // millisecunde Ã®ntre frame-uri
```

**ModificÄƒri:**
- `50` - AnimaÈ›ie rapidÄƒ (20 fps)
- `200` - AnimaÈ›ie lentÄƒ pentru prezentÄƒri (5 fps)
- `500` - Super lent pentru debugging

### 4. Scheme de Culori Alternative

Ãn funcÈ›ia `drawNeuron` (linia ~250):

**Gradient roÈ™u-verde:**
```javascript
const intensity = Math.floor(value * 255);
ctx.fillStyle = `rgb(${255 - intensity}, ${intensity}, 0)`;
```

**Gradient cyan-magenta:**
```javascript
ctx.fillStyle = `rgb(0, ${intensity}, ${255 - intensity})`;
```

**Heatmap style:**
```javascript
// Albastru â†’ Galben â†’ RoÈ™u
if (value < 0.5) {
  ctx.fillStyle = `rgb(0, ${value * 510}, 255)`;
} else {
  ctx.fillStyle = `rgb(${(value - 0.5) * 510}, 255, ${255 - (value - 0.5) * 510})`;
}
```

---

## ğŸ“Š ÃnÈ›elegerea Metricilor

### Loss (Mean Squared Error)

**Formula:**
```
Loss = Î£(target - prediction)Â² / n
```

**EvoluÈ›ie tipicÄƒ:**
```
Epoca 0:     Loss = 1.000  (total random)
Epoca 100:   Loss = 0.450  (learning...)
Epoca 500:   Loss = 0.120  (almost there)
Epoca 1000:  Loss = 0.015  (excellent!)
Epoca 2000:  Loss = 0.002  (nearly perfect)
```

**Interpretare:**
- **>0.5:** ReÈ›eaua Ã®ncÄƒ Ã®nvaÈ›Äƒ fundamental pattern-ul
- **0.1-0.5:** Pattern Ã®nvÄƒÈ›at, se rafineazÄƒ detalii
- **<0.1:** PerformanÈ›Äƒ bunÄƒ, predicÈ›ii fiabile
- **<0.01:** Excelent, aproape perfect pentru XOR

### Learning Rate - Efecte

| Valoare | Efect | Recomandare |
|---------|-------|------------|
| 0.01-0.1 | Lent dar sigur | Pentru probleme complexe |
| 0.3-0.5 | Echilibrat | **OPTIM** pentru XOR |
| 0.6-0.8 | Rapid dar instabil | DacÄƒ timpul e scurt |
| 0.9-1.0 | Risc divergenÈ›Äƒ | âš ï¸ EvitÄƒ! |

**Simptome learning rate prea mare:**
- Loss oscileazÄƒ wild
- NiciodatÄƒ nu convergeÈ™te
- PredicÈ›ii inconsistente

**Simptome learning rate prea mic:**
- Loss scade foarte lent
- NecesitÄƒ 10,000+ epoci
- Dar eventual convergeÈ™te perfect

---

## ğŸ“ Concepte Deep Learning Demonstrate

### 1. **Universal Approximation Theorem**
Orice funcÈ›ie continuÄƒ poate fi aproximatÄƒ de o reÈ›ea neuralÄƒ cu un singur strat ascuns suficient de mare.

**Demonstrat:** XOR (funcÈ›ie non-liniarÄƒ) Ã®nvÄƒÈ›atÄƒ cu 4 neuroni hidden.

### 2. **Non-linearitatea este EsenÈ›ialÄƒ**
Un perceptron simplu (fÄƒrÄƒ hidden layer) NU poate Ã®nvÄƒÈ›a XOR.

**Proof:** ÃncearcÄƒ cu `(2, 0, 1)` - fail garantat!

### 3. **Gradient Descent FuncÈ›ioneazÄƒ**
Ajustare iterativÄƒ bazatÄƒ pe gradienÈ›i converge cÄƒtre minimum local.

**Observat:** Loss scade constant, nu random.

### 4. **Hiperparametri ImportÄƒ**
Learning rate schimbÄƒ dramatic comportamentul.

**ExperimenteazÄƒ:** 0.1 vs 0.9 - vezi diferenÈ›a!

### 5. **Vizualizarea AjutÄƒ IntuiÈ›ia**
Matematica devine clarÄƒ cÃ¢nd vezi vizual ce se Ã®ntÃ¢mplÄƒ.

**Impact:** De la formule abstracte la Ã®nÈ›elegere profundÄƒ.

---

## ğŸ› Troubleshooting

### âŒ AplicaÈ›ia nu porneÈ™te

**Eroare:** `npm: command not found`
```bash
# InstaleazÄƒ Node.js È™i npm
# Ubuntu/Debian:
sudo apt install nodejs npm

# macOS:
brew install node

# Windows: descarcÄƒ de pe nodejs.org
```

**Eroare:** `EACCES: permission denied`
```bash
# RuleazÄƒ cu sudo (Linux/Mac)
sudo npm install

# Sau schimbÄƒ permisiuni
sudo chown -R $USER ~/.npm
```

### âŒ DependenÈ›ele nu se instaleazÄƒ

```bash
# È˜terge cache È™i reinstaleazÄƒ
rm -rf node_modules package-lock.json
npm cache clean --force
npm install
```

### âŒ AplicaÈ›ia e lentÄƒ/lag

**Cauze posibile:**
1. Browser vechi â†’ Update Chrome/Firefox
2. Multe tab-uri deschise â†’ Ãnchide restul
3. AnimaÈ›ii prea rapide â†’ CreÈ™te intervalul la 200ms

**SoluÈ›ie:** Reduce neuroni hidden de la 4 la 3 sau 2.

### âŒ Loss nu scade

**Debugging:**
1. ReseteazÄƒ reÈ›eaua (ğŸ”„ Reset)
2. VerificÄƒ learning rate (0.3-0.7 ideal)
3. LasÄƒ sÄƒ ruleze >1000 epoci
4. DacÄƒ persistÄƒ, verificÄƒ codul pentru erori

### âŒ AnimaÈ›iile nu se vÄƒd

**VerificÄƒ:**
1. Hardware acceleration Ã®n browser activat
2. Canvas rendering support (F12 console)
3. JavaScript errors (F12 console â†’ Console tab)

---

## ğŸ“š Resurse Suplimentare

### Pentru ÃnvÄƒÈ›are ContinuÄƒ:

**CÄƒrÈ›i:**
- "Neural Networks and Deep Learning" - Michael Nielsen (gratuit online)
- "Deep Learning" - Goodfellow, Bengio, Courville
- "Grokking Deep Learning" - Andrew Trask (foarte vizual!)

**Cursuri Online:**
- 3Blue1Brown - Neural Networks series (YouTube)
- Fast.ai - Practical Deep Learning (gratuit)
- Coursera - Deep Learning Specialization (Andrew Ng)

**Playground-uri Interactive:**
- TensorFlow Playground - playground.tensorflow.org
- Neural Network Playground - teachablemachine.withgoogle.com
- ConvNetJS - cs.stanford.edu/people/karpathy/convnetjs

---

## ğŸŒŸ Features Viitoare (Idei)

DacÄƒ vrei sÄƒ extinzi aplicaÈ›ia:

- [ ] **Training history chart** - grafic loss pe epoci
- [ ] **Multiple datasets** - XOR, AND, OR, spirale
- [ ] **Compare architectures** - 2-3-1 vs 2-4-1 vs 2-8-1
- [ ] **3D visualization** - pentru > 2 inputs
- [ ] **Export video** - salveazÄƒ antrenarea ca GIF
- [ ] **Weight editing** - modificÄƒ manual greutÄƒÈ›ile
- [ ] **Confusion matrix** - pentru clasificare
- [ ] **Batch training** - antreneazÄƒ pe loturi

---

## ğŸ¤ Contributori

**Autori:** Bianca-Maria Abbasi Pazeyazd & Octavian Mihai  
**Universitate:** RomÃ¢no-AmericanÄƒ  
**Facultate:** InformaticÄƒ ManagerialÄƒ  
**Data:** Decembrie 2024  

**Stack Tehnologic:**
- React 18
- Vite 6
- Canvas API
- JavaScript ES6+
- Pure CSS (fÄƒrÄƒ Bootstrap/Tailwind)

**Implementare:** 100% de la zero, fÄƒrÄƒ TensorFlow/PyTorch!

---

## ğŸ“ Suport

Probleme sau Ã®ntrebÄƒri?
- ğŸ“– DocumentaÈ›ie React: https://react.dev
- ğŸ”§ Vite Docs: https://vitejs.dev
- ğŸ’¬ Stack Overflow: tag `react` + `neural-network`

---

## ğŸ“œ LicenÈ›Äƒ

MIT License - Free pentru uz educaÈ›ional È™i comercial.

---

**ğŸš€ Mult Succes cu Lucrarea!**

*AceastÄƒ aplicaÈ›ie transformÄƒ concepte abstracte Ã®n experienÈ›Äƒ vizualÄƒ È™i interactivÄƒ - perfect pentru a demonstra Ã®nÈ›elegerea profundÄƒ a deep learning!* ğŸ§ âœ¨
